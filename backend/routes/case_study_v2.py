"""
Case Study v2 Routes
PU Learning Workbench - Advanced experimental workbench routes

=== DATABASE DATETIME FORMAT RULE ===
All datetime fields in SQLite database MUST use the following format:
- Format: 'YYYY-MM-DD HH:MM:SS' (e.g., '2025-08-25 23:20:34')
- Generated by: datetime.now().strftime('%Y-%m-%d %H:%M:%S')
- Compatible with: Prisma ORM, SQLite DATETIME type
- DO NOT USE: ISO format with 'T' separator or timezone info
==========================================
"""

from fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect
from typing import List, Dict, Any, Optional
import asyncio
import json
import uuid
import logging
from datetime import datetime
import sys
import os

# Configure logging
logger = logging.getLogger(__name__)

# Create router
case_study_v2_router = APIRouter(prefix="/api/v2", tags=["Case Study v2"])

# Global variables for services (initialized later)
db_manager = None
candidate_generator = None
model_trainer = None
model_evaluator = None
websocket_manager = None

def get_current_datetime():
    """
    Returns current datetime in SQLite-compatible format
    Format: 'YYYY-MM-DD HH:MM:SS' (e.g., '2025-08-25 23:20:34')
    """
    from datetime import datetime
    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

def _import_case_study_v2_modules():
    """Dynamically import case-study-v2 modules"""
    try:
        # Import the services from the new location
        from services.case_study_v2 import (
            DatabaseManager,
            CandidateGenerator,
            ModelTrainer,
            ModelEvaluator,
            WebSocketManager
        )
        logger.info("Successfully imported case study v2 modules")
        return {
            'DatabaseManager': DatabaseManager,
            'CandidateGenerator': CandidateGenerator,
            'ModelTrainer': ModelTrainer,
            'ModelEvaluator': ModelEvaluator,
            'WebSocketManager': WebSocketManager
        }
    except ImportError as e:
        logger.error(f"Failed to import case-study-v2 modules: {e}")
        return None

async def init_case_study_v2():
    """Initialize Case Study v2 services"""
    global db_manager, candidate_generator, model_trainer, model_evaluator, websocket_manager

    try:
        # Import modules
        modules = _import_case_study_v2_modules()
        if not modules:
            logger.warning("Case Study v2 modules not available, using fallback responses")
            return False

        # Initialize database manager
        db_manager = modules['DatabaseManager']()
        await db_manager.connect()

        # Initialize services
        candidate_generator = modules['CandidateGenerator'](db_manager)
        model_trainer = modules['ModelTrainer'](db_manager)
        model_evaluator = modules['ModelEvaluator'](db_manager)
        websocket_manager = modules['WebSocketManager']()

        logger.info("Case Study v2 services initialized successfully")
        return True

    except Exception as e:
        logger.error(f"Failed to initialize Case Study v2 services: {e}")
        # Still allow fallback functionality
        logger.warning("Continuing with fallback implementation")
        return False

async def cleanup_case_study_v2():
    """Cleanup Case Study v2 services"""
    global db_manager
    if db_manager:
        try:
            await db_manager.disconnect()
            logger.info("Case Study v2 services cleanup complete")
        except Exception as e:
            logger.error(f"Error during Case Study v2 cleanup: {e}")

# ========== API Routes ==========

@case_study_v2_router.post("/experiment-runs")
async def create_experiment_run(request: dict):
    """Creates a new ExperimentRun and initiates candidate generation"""
    try:
        # Use direct SQLite connection
        import sqlite3
        import uuid
        from datetime import datetime

        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # Generate new ID
        experiment_id = str(uuid.uuid4())
        current_time = get_current_datetime()  # Use unified datetime format

        # Create experiment run
        cursor.execute('''
            INSERT INTO experiment_run (
                id, name, description, filtering_parameters, status,
                candidate_count, positive_label_count, negative_label_count,
                created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            experiment_id,
            request.get('name', 'Unnamed Experiment'),
            request.get('description'),
            json.dumps(request.get('filtering_parameters', {})),
            'CONFIGURING',
            0, 0, 0,
            current_time,
            current_time
        ))

        conn.commit()
        conn.close()

        return {
            'id': experiment_id,
            'name': request.get('name', 'Unnamed Experiment'),
            'description': request.get('description'),
            'status': 'CONFIGURING',
            'candidateCount': 0,
            'createdAt': current_time
        }

    except Exception as e:
        logger.error(f"Error creating experiment run: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.post("/generate-candidates")
async def generate_candidates(request: dict):
    """Generates anomaly candidates based on filter parameters"""
    try:
        # 解析請求參數
        filter_params = request.get("filter_params", {})
        save_labels = request.get("save_labels", False)  # 新增參數：是否保存為實際標籤

        logger.info(f"[GENERATE_CANDIDATES] save_labels 參數: {save_labels}")

        # 提取過濾參數
        selected_dataset_ids = filter_params.get("selectedDatasetIds", [])
        buildings = filter_params.get("buildings", [])
        floors = filter_params.get("floors", [])
        rooms = filter_params.get("rooms", [])
        occupant_types = filter_params.get("occupantTypes", [])

        # 異常檢測參數
        z_score_threshold = filter_params.get("zScoreThreshold", 2.5)
        spike_threshold = filter_params.get("spikeThreshold", 200)
        min_event_duration = filter_params.get("minEventDuration", 30)

        # 時間範圍
        start_date = filter_params.get("startDate", "")
        start_time = filter_params.get("startTime", "")
        end_date = filter_params.get("endDate", "")
        end_time = filter_params.get("endTime", "")

        import sqlite3
        import json
        import numpy as np

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 建立基礎查詢條件
        where_conditions = []
        params = []

        # 資料集過濾
        if selected_dataset_ids:
            placeholders = ','.join(['?' for _ in selected_dataset_ids])
            where_conditions.append(f"id IN ({placeholders})")
            params.extend(selected_dataset_ids)

        # 建築物過濾
        if buildings:
            placeholders = ','.join(['?' for _ in buildings])
            where_conditions.append(f"building IN ({placeholders})")
            params.extend(buildings)

        # 樓層過濾
        if floors:
            placeholders = ','.join(['?' for _ in floors])
            where_conditions.append(f"floor IN ({placeholders})")
            params.extend(floors)

        # 房間過濾
        if rooms:
            placeholders = ','.join(['?' for _ in rooms])
            where_conditions.append(f"room IN ({placeholders})")
            params.extend(rooms)

        # 佔用者類型過濾
        if occupant_types:
            placeholders = ','.join(['?' for _ in occupant_types])
            where_conditions.append(f"occupant_type IN ({placeholders})")
            params.extend(occupant_types)

        # 構建查詢
        base_query = "SELECT * FROM analysis_datasets"
        if where_conditions:
            base_query += " WHERE " + " AND ".join(where_conditions)

        cursor.execute(base_query, params)
        filtered_datasets = cursor.fetchall()

        # 使用真實數據進行異常檢測來計算候選數量
        total_candidates = 0
        anomaly_events_to_create = []  # 用於保存異常事件數據

        if filtered_datasets:
            import numpy as np

            # 獲取數據集列名
            columns = [desc[0] for desc in cursor.description]

            for dataset_row in filtered_datasets:
                dataset = dict(zip(columns, dataset_row))
                dataset_id = dataset['id']

                logger.info(f"[GENERATE_CANDIDATES] 分析數據集: {dataset['name']}")

                # 查詢該數據集的實際電力數據
                cursor.execute('''
                    SELECT timestamp, wattage_total, wattage_110v, wattage_220v
                    FROM analysis_ready_data
                    WHERE dataset_id = ?
                    ORDER BY timestamp
                ''', (dataset_id,))

                power_data = cursor.fetchall()

                if not power_data:
                    logger.warning(f"[GENERATE_CANDIDATES] 數據集 {dataset_id} 沒有實際數據")
                    continue

                # 轉換為 numpy arrays 進行分析
                timestamps = [row[0] for row in power_data]
                wattage_total = np.array([row[1] for row in power_data])
                wattage_110v = np.array([row[2] for row in power_data])
                wattage_220v = np.array([row[3] for row in power_data])

                # 建立異常標記數組，避免重複計算
                anomaly_mask = np.zeros(len(wattage_total), dtype=bool)

                logger.info(f"[GENERATE_CANDIDATES] 數據集總數據點: {len(wattage_total)}")

                # 1. Z-score 異常檢測
                z_anomaly_count = 0
                if len(wattage_total) > 1:
                    # 計算總功率的 Z-score
                    mean_power = np.mean(wattage_total)
                    std_power = np.std(wattage_total)

                    if std_power > 0:
                        z_scores = np.abs((wattage_total - mean_power) / std_power)
                        z_anomaly_mask = z_scores > z_score_threshold
                        z_anomaly_count = np.sum(z_anomaly_mask)

                        # 添加到總異常標記
                        anomaly_mask |= z_anomaly_mask

                        logger.info(f"[GENERATE_CANDIDATES] Z-score 異常 (閾值 {z_score_threshold}): {z_anomaly_count}")

                # 2. 功率峰值檢測（使用相對閾值）
                # 計算相對於平均功率的峰值閾值
                mean_power = np.mean(wattage_total)
                # spike_threshold 是百分比，轉換為絕對閾值
                absolute_spike_threshold = mean_power * (1 + spike_threshold / 100.0)

                spike_anomaly_mask = wattage_total > absolute_spike_threshold
                spike_anomaly_count = np.sum(spike_anomaly_mask)

                # 添加到總異常標記
                anomaly_mask |= spike_anomaly_mask

                logger.info(f"[GENERATE_CANDIDATES] 功率峰值 (相對閾值 {spike_threshold}%, 絕對閾值 {absolute_spike_threshold:.1f}W): {spike_anomaly_count}")

                # 3. 連續性檢測 - 檢測數據跳躍
                jump_anomaly_count = 0
                if len(wattage_total) > 1:
                    power_diff = np.abs(np.diff(wattage_total))
                    diff_threshold = np.mean(wattage_total) * 0.5  # 50% 功率變化閾值

                    # 為 diff 創建與原數組長度相同的 mask（最後一個元素設為 False）
                    jump_anomaly_mask = np.zeros(len(wattage_total), dtype=bool)
                    jump_anomaly_mask[:-1] = power_diff > diff_threshold

                    jump_anomaly_count = np.sum(jump_anomaly_mask)

                    # 添加到總異常標記
                    anomaly_mask |= jump_anomaly_mask

                    logger.info(f"[GENERATE_CANDIDATES] 功率跳躍異常: {jump_anomaly_count}")

                # 計算聯集後的總異常數量
                total_anomalies = np.sum(anomaly_mask)

                # 4. 最小事件持續時間過濾
                # 將連續的異常點聚合成事件
                # 簡化實現：根據最小持續時間要求來減少候選數量

                # 持續時間過濾：較長的持續時間要求應該產生較少的候選
                # 假設每分鐘一個數據點，基準為 30 分鐘
                duration_factor = min(1.0, 30.0 / max(1, min_event_duration))
                dataset_candidates = int(total_anomalies * duration_factor)

                # 確保候選數量不超過原始數據點數量的合理比例（最多 10%）
                max_reasonable_candidates = max(1, int(len(wattage_total) * 0.1))
                dataset_candidates = min(dataset_candidates, max_reasonable_candidates)

                total_candidates += max(0, dataset_candidates)

                # 如果需要保存標籤，準備異常事件數據
                if save_labels and dataset_candidates > 0:
                    logger.info(f"[GENERATE_CANDIDATES] 準備創建異常事件，save_labels={save_labels}, dataset_candidates={dataset_candidates}")
                    # 找到異常的時間點
                    anomaly_indices = np.where(anomaly_mask)[0]
                    logger.info(f"[GENERATE_CANDIDATES] 找到 {len(anomaly_indices)} 個異常索引")

                    # 根據 duration_factor 選擇部分異常點作為候選事件
                    if len(anomaly_indices) > dataset_candidates:
                        # 隨機選擇或按重要性選擇
                        selected_indices = np.random.choice(
                            anomaly_indices,
                            size=dataset_candidates,
                            replace=False
                        )
                    else:
                        selected_indices = anomaly_indices

                    logger.info(f"[GENERATE_CANDIDATES] 選擇了 {len(selected_indices)} 個異常點作為事件")

                    # 為每個選中的異常點創建事件數據
                    for idx in selected_indices:
                        anomaly_events_to_create.append({
                            'meter_id': f"{dataset['name']}_L1",  # 使用數據集名稱作為meter_id
                            'event_timestamp': timestamps[idx],
                            'wattage_total': float(wattage_total[idx]),
                            'wattage_110v': float(wattage_110v[idx]),
                            'wattage_220v': float(wattage_220v[idx]),
                            'detection_rule': f"Z-score:{z_score_threshold},Spike:{spike_threshold}%,Duration:{min_event_duration}min",
                            'score': 0.8,  # 預設信心分數
                            'data_window': f"[{timestamps[max(0, idx-5)]} - {timestamps[min(len(timestamps)-1, idx+5)]}]"
                        })

                    logger.info(f"[GENERATE_CANDIDATES] 總共準備創建 {len(anomaly_events_to_create)} 個異常事件")

                logger.info(f"[GENERATE_CANDIDATES] Z-score: {z_anomaly_count}, 峰值: {spike_anomaly_count}, 跳躍: {jump_anomaly_count}")
                logger.info(f"[GENERATE_CANDIDATES] 聯集後總異常: {total_anomalies}, 持續時間調整後: {dataset_candidates}")
                logger.info(f"[GENERATE_CANDIDATES] 數據集 {dataset['name']} 最終候選數: {dataset_candidates}")

        logger.info(f"[GENERATE_CANDIDATES] 總候選數: {total_candidates}")

        # 如果只是預覽模式，不創建實驗運行記錄，直接返回結果
        if not save_labels:
            conn.commit()
            conn.close()

            logger.info(f"[GENERATE_CANDIDATES] 預覽模式完成，總候選數: {total_candidates}")
            return {
                "success": True,
                "candidate_count": total_candidates,
                "status": "preview",
                "message": f"Preview completed. Found {total_candidates} potential candidates."
            }

        # 只有在 save_labels=True 時才創建實驗運行記錄
        from datetime import datetime
        experiment_run_id = str(uuid.uuid4())
        current_timestamp = get_current_datetime()  # Use unified datetime format

        # 構建實驗名稱
        experiment_name = f"Candidate Generation - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        experiment_name += " [LABELING]"  # 標籤模式總是標記為 LABELING
        if buildings:
            experiment_name += f" - Buildings: {', '.join(buildings[:2])}{'...' if len(buildings) > 2 else ''}"

        # 標籤模式的狀態總是 LABELING
        status = "LABELING"

        cursor.execute('''
            INSERT INTO experiment_run
            (id, name, description, filtering_parameters, status, candidate_count,
             positive_label_count, negative_label_count, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            experiment_run_id,
            experiment_name,
            f"Generated {total_candidates} anomaly candidates using filter criteria",
            json.dumps(filter_params, sort_keys=True),  # 確保與查詢時的格式一致
            status,
            total_candidates,
            0,  # 初始時沒有標籤
            0,
            current_timestamp,
            current_timestamp
        ))

        # 創建實際的 anomaly_event 記錄
        anomaly_events_created = 0
        if anomaly_events_to_create:
            logger.info(f"[GENERATE_CANDIDATES] 開始創建 {len(anomaly_events_to_create)} 個異常事件記錄")

            # 先刪除具有完全相同過濾參數且狀態為 'LABELING' 的舊實驗運行及其異常事件
            # 注意：只有參數完全匹配時才清理，避免誤刪其他實驗
            filter_params_json = json.dumps(filter_params, sort_keys=True)  # 確保JSON鍵值順序一致
            cursor.execute('''
                SELECT id, name FROM experiment_run
                WHERE filtering_parameters = ? AND status = 'LABELING'
                ORDER BY created_at DESC
            ''', (filter_params_json,))

            existing_labeling_runs = cursor.fetchall()

            if existing_labeling_runs:
                logger.info(f"[GENERATE_CANDIDATES] 找到 {len(existing_labeling_runs)} 個具有相同參數的LABELING實驗")
                for run in existing_labeling_runs:
                    logger.info(f"[GENERATE_CANDIDATES] 將清理實驗: {run[0]} - {run[1]}")

                run_ids_to_clean = [run[0] for run in existing_labeling_runs]
                placeholders = ','.join(['?' for _ in run_ids_to_clean])

                # 先刪除異常事件
                delete_events_query = f'''
                    DELETE FROM anomaly_event
                    WHERE experiment_run_id IN ({placeholders})
                '''
                cursor.execute(delete_events_query, run_ids_to_clean)
                deleted_events_count = cursor.rowcount

                # 再刪除舊的實驗運行記錄
                delete_runs_query = f'''
                    DELETE FROM experiment_run
                    WHERE id IN ({placeholders})
                '''
                cursor.execute(delete_runs_query, run_ids_to_clean)
                deleted_runs_count = cursor.rowcount

                logger.info(f"[GENERATE_CANDIDATES] 清理舊資料: 刪除了 {deleted_events_count} 個異常事件和 {deleted_runs_count} 個實驗運行記錄")
            else:
                logger.info(f"[GENERATE_CANDIDATES] 沒有找到需要清理的舊實驗記錄")

            for event_data in anomaly_events_to_create:
                anomaly_event_id = str(uuid.uuid4())
                event_id = f"AUTO_{anomaly_event_id[:8]}"
                cursor.execute('''
                    INSERT INTO anomaly_event
                    (id, event_id, meter_id, event_timestamp, detection_rule, score,
                     data_window, status, experiment_run_id, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    anomaly_event_id,
                    event_id,
                    event_data['meter_id'],
                    event_data['event_timestamp'],
                    event_data['detection_rule'],
                    event_data['score'],
                    event_data['data_window'],
                    'UNREVIEWED',
                    experiment_run_id,
                    current_timestamp,
                    current_timestamp
                ))
                anomaly_events_created += 1

            logger.info(f"[GENERATE_CANDIDATES] 創建了 {anomaly_events_created} 個異常事件記錄")

        conn.commit()
        conn.close()

        logger.info(f"Generated {total_candidates} candidates and created {anomaly_events_created} anomaly events for experiment {experiment_run_id}")

        response = {
            "success": True,
            "experiment_run_id": experiment_run_id,
            "candidate_count": total_candidates,
            "anomaly_events_created": anomaly_events_created,
            "status": "LABELING",
            "filtered_datasets_count": len(filtered_datasets),
            "filter_summary": {
                "buildings": buildings,
                "floors": floors,
                "rooms": rooms,
                "occupant_types": occupant_types,
                "z_score_threshold": z_score_threshold,
                "spike_threshold": spike_threshold
            },
            "message": f"Successfully generated {total_candidates} anomaly candidates and created {anomaly_events_created} anomaly events for labeling"
        }

        return response

    except Exception as e:
        logger.error(f"Error generating candidates: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.get("/experiment-runs")
async def get_experiment_runs():
    """Retrieves a list of all ExperimentRun records"""
    try:
        # Use direct SQLite connection
        import sqlite3

        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # Query all experiment runs
        cursor.execute('''
            SELECT id, name, description, filtering_parameters, status,
                   candidate_count, positive_label_count, negative_label_count,
                   created_at, updated_at, candidate_stats
            FROM experiment_run
            ORDER BY created_at DESC
            LIMIT 100
        ''')

        rows = cursor.fetchall()
        conn.close()

        # Format response
        result = []
        for row in rows:
            # Parse JSON fields
            filtering_parameters = {}
            candidate_stats = {}

            try:
                if row[3]:  # filtering_parameters
                    filtering_parameters = json.loads(row[3])
            except json.JSONDecodeError:
                pass

            try:
                if row[10]:  # candidate_stats
                    candidate_stats = json.loads(row[10])
            except json.JSONDecodeError:
                pass

            result.append({
                'id': row[0],
                'name': row[1],
                'description': row[2],
                'filteringParameters': filtering_parameters,
                'status': row[4],
                'candidateCount': row[5],
                'positiveLabelCount': row[6],
                'negativeLabelCount': row[7],
                'createdAt': row[8],
                'updatedAt': row[9],
                'candidateStats': candidate_stats,
                'anomalyEvents': [],
                'trainedModels': []
            })

        return result

    except Exception as e:
        logger.error(f"Error fetching experiment runs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
@case_study_v2_router.get("/experiment-runs/{run_id}")
async def get_experiment_run(run_id: str):
    """Retrieves a specific ExperimentRun by ID"""
    try:
        # 直接使用主資料庫讀取特定的 experiment_run
        import sqlite3
        import json

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 查詢特定實驗運行
        cursor.execute('''
            SELECT id, name, description, filtering_parameters, status,
                   candidate_count, positive_label_count, negative_label_count,
                   created_at, updated_at, candidate_stats
            FROM experiment_run
            WHERE id = ?
        ''', (run_id,))

        row = cursor.fetchone()

        if not row:
            raise HTTPException(status_code=404, detail="Experiment run not found")

        # 解析 JSON 欄位
        filtering_parameters = {}
        candidate_stats = {}

        try:
            if row[3]:  # filtering_parameters
                filtering_parameters = json.loads(row[3])
        except json.JSONDecodeError:
            pass

        try:
            if row[10]:  # candidate_stats
                candidate_stats = json.loads(row[10])
        except json.JSONDecodeError:
            pass

        # 轉換時間戳記
        from datetime import datetime
        created_at = None
        updated_at = None

        if row[8]:  # created_at
            try:
                created_at = datetime.fromtimestamp(row[8] / 1000).isoformat() + "Z"
            except:
                created_at = str(row[8])

        if row[9]:  # updated_at
            try:
                # 如果是 ISO 字符串格式，直接使用
                if isinstance(row[9], str) and ('T' in row[9] or 'Z' in row[9]):
                    updated_at = row[9]
                else:
                    # 如果是時間戳，則轉換
                    updated_at = datetime.fromtimestamp(float(row[9]) / 1000).isoformat() + "Z"
            except:
                updated_at = str(row[9])

        result = {
            "id": row[0],
            "name": row[1],
            "description": row[2],
            "filtering_parameters": filtering_parameters,
            "status": row[4] or "UNKNOWN",
            "candidate_count": row[5],
            "positive_label_count": row[6],
            "negative_label_count": row[7],
            "created_at": created_at,
            "updated_at": updated_at,
            "candidate_stats": candidate_stats
        }

        conn.close()

        logger.info(f"Successfully retrieved experiment run: {run_id}")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching experiment run {run_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.delete("/experiment-runs/{run_id}")
async def delete_experiment_run(run_id: str):
    """Delete an experiment run and all associated data"""
    try:
        import sqlite3

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 檢查實驗是否存在
        cursor.execute('SELECT id FROM experiment_run WHERE id = ?', (run_id,))
        if not cursor.fetchone():
            conn.close()
            raise HTTPException(status_code=404, detail="Experiment run not found")

        # 由於 SQLite 支援級聯刪除，我們需要手動刪除相關資料
        # 按照依賴順序刪除：ModelPrediction -> EvaluationRun -> TrainedModel -> EventLabelLink -> AnomalyEvent -> ExperimentRun

        # 1. 刪除 ModelPrediction (透過 EvaluationRun 關聯)
        cursor.execute('''
            DELETE FROM model_predictions
            WHERE evaluation_run_id IN (
                SELECT er.id FROM evaluation_runs er
                JOIN trained_models tm ON er.trained_model_id = tm.id
                WHERE tm.experiment_run_id = ?
            )
        ''', (run_id,))

        # 2. 刪除 EvaluationRun (透過 TrainedModel 關聯)
        cursor.execute('''
            DELETE FROM evaluation_runs
            WHERE trained_model_id IN (
                SELECT id FROM trained_models WHERE experiment_run_id = ?
            )
        ''', (run_id,))

        # 3. 刪除 TrainedModel
        cursor.execute('DELETE FROM trained_models WHERE experiment_run_id = ?', (run_id,))

        # 4. 刪除 EventLabelLink (透過 AnomalyEvent 關聯)
        cursor.execute('''
            DELETE FROM event_label_link
            WHERE event_id IN (
                SELECT id FROM anomaly_event WHERE experiment_run_id = ?
            )
        ''', (run_id,))

        # 5. 刪除 AnomalyEvent
        cursor.execute('DELETE FROM anomaly_event WHERE experiment_run_id = ?', (run_id,))

        # 6. 最後刪除 ExperimentRun
        cursor.execute('DELETE FROM experiment_run WHERE id = ?', (run_id,))

        # 提交所有變更
        conn.commit()
        conn.close()

        logger.info(f"Successfully deleted experiment run: {run_id}")
        return {"success": True, "message": "Experiment run and all associated data deleted successfully"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting experiment run {run_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to delete experiment run: {str(e)}")

@case_study_v2_router.get("/experiment-runs/{run_id}/candidates")
async def get_experiment_candidates(run_id: str):
    """Fetches all AnomalyEvent candidates for a given ExperimentRun"""
    try:
        # 直接使用主資料庫讀取異常事件候選
        import sqlite3
        import json

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 查詢指定實驗運行的異常事件
        cursor.execute('''
            SELECT id, event_id, meter_id, event_timestamp, detection_rule, score,
                   data_window, status, reviewer_id, review_timestamp, justification_notes,
                   created_at, updated_at, experiment_run_id
            FROM anomaly_event
            WHERE experiment_run_id = ?
            ORDER BY event_timestamp DESC
        ''', (run_id,))

        rows = cursor.fetchall()

        # 轉換為字典格式
        result = []
        for row in rows:
            # 解析 JSON 欄位
            data_window = {}
            try:
                if row[6]:  # data_window
                    data_window = json.loads(row[6])
            except json.JSONDecodeError:
                pass

            # 轉換時間戳記
            from datetime import datetime
            event_timestamp = None
            review_timestamp = None
            created_at = None
            updated_at = None

            if row[3]:  # event_timestamp
                try:
                    if isinstance(row[3], str):
                        event_timestamp = row[3]
                    else:
                        event_timestamp = datetime.fromtimestamp(row[3] / 1000).isoformat() + "Z"
                except:
                    event_timestamp = str(row[3])

            if row[9]:  # review_timestamp
                try:
                    if isinstance(row[9], str):
                        review_timestamp = row[9]
                    else:
                        review_timestamp = datetime.fromtimestamp(row[9] / 1000).isoformat() + "Z"
                except:
                    review_timestamp = str(row[9])

            if row[11]:  # created_at
                try:
                    if isinstance(row[11], str):
                        created_at = row[11]
                    else:
                        created_at = datetime.fromtimestamp(row[11] / 1000).isoformat() + "Z"
                except:
                    created_at = str(row[11])

            if row[12]:  # updated_at
                try:
                    if isinstance(row[12], str):
                        updated_at = row[12]
                    else:
                        updated_at = datetime.fromtimestamp(row[12] / 1000).isoformat() + "Z"
                except:
                    updated_at = str(row[12])

            result.append({
                "id": row[0],
                "event_id": row[1],
                "meter_id": row[2],
                "event_timestamp": event_timestamp,
                "detection_rule": row[4],
                "score": row[5],
                "data_window": data_window,
                "status": row[7] or "PENDING",
                "reviewer_id": row[8],
                "review_timestamp": review_timestamp,
                "justification_notes": row[10],
                "created_at": created_at,
                "updated_at": updated_at,
                "experiment_run_id": row[13]
            })

        conn.close()

        logger.info(f"Successfully retrieved {len(result)} candidates for experiment {run_id}")
        return result

    except Exception as e:
        logger.error(f"Error fetching candidates for experiment {run_id}: {str(e)}")

        # 回傳空陣列作為後援
        logger.warning(f"Falling back to empty array for experiment {run_id}")
        return []

@case_study_v2_router.post("/anomaly-events/{event_id}/label")
async def label_anomaly_event(event_id: str, request: dict):
    """Submits a label for an AnomalyEvent"""
    try:
        # 直接使用主資料庫更新異常事件標籤
        import sqlite3

        # 解析請求參數
        status = request.get("status")
        reviewer_id = request.get("reviewer_id")
        justification_notes = request.get("justification_notes", "")

        if not status or not reviewer_id:
            raise HTTPException(status_code=400, detail="status and reviewer_id are required")

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 檢查異常事件是否存在
        cursor.execute('SELECT id FROM anomaly_event WHERE id = ?', (event_id,))
        if not cursor.fetchone():
            conn.close()
            raise HTTPException(status_code=404, detail="Anomaly event not found")

        # 更新異常事件狀態
        current_timestamp = get_current_datetime()  # Use unified datetime format

        cursor.execute('''
            UPDATE anomaly_event
            SET status = ?, reviewer_id = ?, justification_notes = ?,
                review_timestamp = ?, updated_at = ?
            WHERE id = ?
        ''', (status, reviewer_id, justification_notes, current_timestamp, current_timestamp, event_id))

        if cursor.rowcount == 0:
            conn.close()
            raise HTTPException(status_code=404, detail="Anomaly event not found")

        conn.commit()
        conn.close()

        logger.info(f"Successfully labeled anomaly event {event_id} with status {status}")
        return {
            "success": True,
            "message": "Event labeled successfully",
            "event_id": event_id,
            "status": status,
            "reviewer_id": reviewer_id
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error labeling event {event_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@case_study_v2_router.post("/training-jobs")
async def start_training_job(request: dict):
    """Starts a new model training job"""
    try:
        # 直接使用主資料庫創建訓練作業
        import sqlite3
        import json

        # 解析請求參數
        model_name = request.get("model_name")
        scenario_type = request.get("scenario_type")
        experiment_run_id = request.get("experiment_run_id")
        training_config = request.get("training_config", {})
        data_source_config = request.get("data_source_config", {})

        if not model_name or not scenario_type or not experiment_run_id:
            raise HTTPException(status_code=400, detail="model_name, scenario_type, and experiment_run_id are required")

        # 生成作業 ID
        job_id = str(uuid.uuid4())

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 檢查實驗運行是否存在
        cursor.execute('SELECT id FROM experiment_run WHERE id = ?', (experiment_run_id,))
        if not cursor.fetchone():
            conn.close()
            raise HTTPException(status_code=404, detail="Experiment run not found")

        # 創建訓練模型記錄
        current_timestamp = get_current_datetime()
        trained_model_id = str(uuid.uuid4())

        cursor.execute('''
            INSERT INTO trained_models
            (id, name, scenario_type, status, experiment_run_id, model_config,
             data_source_config, created_at, job_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            trained_model_id,
            model_name,
            scenario_type,
            "PENDING",
            experiment_run_id,
            json.dumps(training_config),
            json.dumps(data_source_config),
            current_timestamp,
            job_id
        ))

        conn.commit()
        conn.close()

        # 注意：實際的訓練邏輯需要額外實現
        # 這裡只是創建記錄，實際訓練需要背景任務
        logger.info(f"Training job {job_id} created for model {model_name}")
        logger.warning("Actual model training implementation is not available - job created in PENDING status")

        return {
            "job_id": job_id,
            "trained_model_id": trained_model_id,
            "status": "PENDING",
            "message": "Training job created successfully (actual training requires case-study-v2 modules)"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting training job: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.post("/evaluation-jobs")
async def start_evaluation_job(request: dict):
    """Starts a new model evaluation job"""
    try:
        # 直接使用主資料庫創建評估作業
        import sqlite3
        import json

        # 解析請求參數
        evaluation_name = request.get("evaluation_name")
        scenario_type = request.get("scenario_type")
        trained_model_id = request.get("trained_model_id")
        test_set_source = request.get("test_set_source", {})

        if not evaluation_name or not scenario_type or not trained_model_id:
            raise HTTPException(status_code=400, detail="evaluation_name, scenario_type, and trained_model_id are required")

        # 生成作業 ID
        job_id = str(uuid.uuid4())

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 檢查訓練模型是否存在
        cursor.execute('SELECT id FROM trained_models WHERE id = ?', (trained_model_id,))
        if not cursor.fetchone():
            conn.close()
            raise HTTPException(status_code=404, detail="Trained model not found")

        # 創建評估運行記錄
        current_timestamp = get_current_datetime()
        evaluation_run_id = str(uuid.uuid4())

        cursor.execute('''
            INSERT INTO evaluation_runs
            (id, name, scenario_type, status, trained_model_id, test_set_source,
             created_at, job_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            evaluation_run_id,
            evaluation_name,
            scenario_type,
            "PENDING",
            trained_model_id,
            json.dumps(test_set_source),
            current_timestamp,
            job_id
        ))

        conn.commit()
        conn.close()

        # 注意：實際的評估邏輯需要額外實現
        # 這裡只是創建記錄，實際評估需要背景任務
        logger.info(f"Evaluation job {job_id} created for evaluation {evaluation_name}")
        logger.warning("Actual model evaluation implementation is not available - job created in PENDING status")

        return {
            "job_id": job_id,
            "evaluation_run_id": evaluation_run_id,
            "status": "PENDING",
            "message": "Evaluation job created successfully (actual evaluation requires case-study-v2 modules)"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting evaluation job: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.get("/experiment-runs/{run_id}/history")
async def get_experiment_history(run_id: str):
    """Retrieves the full experiment history for an ExperimentRun"""
    try:
        # 直接使用主資料庫獲取實驗歷史
        import sqlite3
        import json

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 檢查實驗運行是否存在
        cursor.execute('SELECT id, name, description, status FROM experiment_run WHERE id = ?', (run_id,))
        experiment = cursor.fetchone()

        if not experiment:
            conn.close()
            raise HTTPException(status_code=404, detail="Experiment run not found")

        # 獲取異常事件候選
        cursor.execute('''
            SELECT COUNT(*) as total_candidates,
                   SUM(CASE WHEN status = 'POSITIVE' THEN 1 ELSE 0 END) as positive_labels,
                   SUM(CASE WHEN status = 'NEGATIVE' THEN 1 ELSE 0 END) as negative_labels,
                   SUM(CASE WHEN status = 'PENDING' OR status IS NULL THEN 1 ELSE 0 END) as pending_labels
            FROM anomaly_event
            WHERE experiment_run_id = ?
        ''', (run_id,))
        candidate_stats = cursor.fetchone()

        # 獲取訓練模型
        cursor.execute('''
            SELECT id, name, status, model_config, data_source_config,
                   training_metrics, created_at, completed_at
            FROM trained_models
            WHERE experiment_run_id = ?
            ORDER BY created_at DESC
        ''', (run_id,))
        trained_models = cursor.fetchall()

        # 獲取評估運行
        cursor.execute('''
            SELECT er.id, er.name, er.status, er.evaluation_metrics,
                   er.created_at, er.completed_at, tm.name as model_name
            FROM evaluation_runs er
            LEFT JOIN trained_models tm ON er.trained_model_id = tm.id
            WHERE tm.experiment_run_id = ?
            ORDER BY er.created_at DESC
        ''', (run_id,))
        evaluation_runs = cursor.fetchall()

        # 轉換時間戳記的輔助函數
        from datetime import datetime
        def convert_timestamp(ts):
            if not ts:
                return None
            try:
                if isinstance(ts, str):
                    return ts
                else:
                    return datetime.fromtimestamp(ts / 1000).isoformat() + "Z"
            except:
                return str(ts)

        # 構建歷史回應
        history = {
            "experiment_run": {
                "id": experiment[0],
                "name": experiment[1],
                "description": experiment[2],
                "status": experiment[3]
            },
            "candidate_generation": {
                "total_candidates": candidate_stats[0] if candidate_stats[0] else 0,
                "positive_labels": candidate_stats[1] if candidate_stats[1] else 0,
                "negative_labels": candidate_stats[2] if candidate_stats[2] else 0,
                "pending_labels": candidate_stats[3] if candidate_stats[3] else 0
            },
            "trained_models": [],
            "evaluation_runs": []
        }

        # 添加訓練模型資訊
        for model in trained_models:
            model_config = {}
            training_metrics = {}

            try:
                if model[3]:  # model_config
                    model_config = json.loads(model[3])
            except json.JSONDecodeError:
                pass

            try:
                if model[5]:  # training_metrics
                    training_metrics = json.loads(model[5])
            except json.JSONDecodeError:
                pass

            history["trained_models"].append({
                "id": model[0],
                "name": model[1],
                "status": model[2],
                "model_config": model_config,
                "training_metrics": training_metrics,
                "created_at": convert_timestamp(model[6]),
                "completed_at": convert_timestamp(model[7])
            })

        # 添加評估運行資訊
        for evaluation in evaluation_runs:
            evaluation_metrics = {}

            try:
                if evaluation[3]:  # evaluation_metrics
                    evaluation_metrics = json.loads(evaluation[3])
            except json.JSONDecodeError:
                pass

            history["evaluation_runs"].append({
                "id": evaluation[0],
                "name": evaluation[1],
                "status": evaluation[2],
                "evaluation_metrics": evaluation_metrics,
                "created_at": convert_timestamp(evaluation[4]),
                "completed_at": convert_timestamp(evaluation[5]),
                "model_name": evaluation[6]
            })

        conn.close()

        logger.info(f"Successfully retrieved experiment history for {run_id}")
        return history

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching experiment history for {run_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.get("/analysis-datasets")
async def get_analysis_datasets():
    """Retrieves a list of available analysis datasets for dataset selection"""
    try:
        # 直接使用 SQLite 連接讀取 AnalysisDataset 資料表
        import sqlite3

        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 查詢所有 AnalysisDataset 記錄
        cursor.execute('''
            SELECT id, name, description, building, floor, room,
                   start_date, end_date, occupant_type, meter_id_l1, meter_id_l2,
                   total_records, positive_labels, created_at
            FROM analysis_datasets
            ORDER BY created_at DESC
        ''')

        rows = cursor.fetchall()
        conn.close()

        # 轉換為字典格式
        result = []
        for row in rows:
            # 處理日期時間欄位
            start_date = row[6]
            end_date = row[7]
            created_at = row[13]

            # 如果是字串格式的日期，保持原樣；如果是時間戳，則轉換
            try:
                from datetime import datetime
                if isinstance(start_date, (int, float)):
                    start_date = datetime.fromtimestamp(start_date).isoformat() + "Z"
                elif isinstance(start_date, str) and not start_date.endswith('Z'):
                    start_date = start_date + "Z" if 'T' in start_date else start_date

                if isinstance(end_date, (int, float)):
                    end_date = datetime.fromtimestamp(end_date).isoformat() + "Z"
                elif isinstance(end_date, str) and not end_date.endswith('Z'):
                    end_date = end_date + "Z" if 'T' in end_date else end_date

                if isinstance(created_at, (int, float)):
                    created_at = datetime.fromtimestamp(created_at).isoformat() + "Z"
                elif isinstance(created_at, str) and not created_at.endswith('Z'):
                    created_at = created_at + "Z" if 'T' in created_at else created_at
            except:
                # 如果轉換失敗，保持原值
                pass

            result.append({
                "id": row[0],
                "name": row[1],
                "description": row[2],
                "building": row[3],
                "floor": row[4],
                "room": row[5],
                "startDate": start_date,
                "endDate": end_date,
                "occupantType": row[8],
                "meterIdL1": row[9],
                "meterIdL2": row[10],
                "totalRecords": row[11],
                "positiveLabels": row[12],
                "createdAt": created_at
            })

        logger.info(f"Successfully retrieved {len(result)} analysis datasets from database")
        return result

    except Exception as e:
        logger.error(f"Error fetching analysis datasets: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve analysis datasets: {str(e)}")

@case_study_v2_router.get("/models")
async def get_available_models():
    """Retrieves a list of trainable models for the selection dropdown"""
    try:
        models = [
            {
                "id": "nnpu_baseline",
                "name": "nnPU Baseline",
                "description": "Non-negative PU Learning with default parameters",
                "parameters": ["learning_rate", "batch_size", "epochs", "prior"]
            },
            {
                "id": "nnpu_advanced",
                "name": "nnPU Advanced",
                "description": "Non-negative PU Learning with custom parameters",
                "parameters": ["learning_rate", "batch_size", "epochs", "prior", "beta", "gamma"]
            },
            {
                "id": "upu",
                "name": "Unbiased PU Learning",
                "description": "Unbiased PU Learning approach",
                "parameters": ["learning_rate", "batch_size", "epochs", "prior"]
            }
        ]
        return models
    except Exception as e:
        logger.error(f"Error fetching available models: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@case_study_v2_router.get("/evaluation-runs/{run_id}")
async def get_evaluation_run(run_id: str):
    """Retrieves detailed results for a single EvaluationRun"""
    try:
        # 直接使用主資料庫讀取評估運行詳情
        import sqlite3
        import json

        # 連接到主資料庫
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 查詢特定評估運行
        cursor.execute('''
            SELECT er.id, er.name, er.scenario_type, er.status, er.trained_model_id,
                   er.test_set_source, er.evaluation_metrics, er.created_at, er.completed_at, er.job_id,
                   tm.name as model_name, tm.model_config
            FROM evaluation_runs er
            LEFT JOIN trained_models tm ON er.trained_model_id = tm.id
            WHERE er.id = ?
        ''', (run_id,))

        row = cursor.fetchone()

        if not row:
            conn.close()
            raise HTTPException(status_code=404, detail="Evaluation run not found")

        # 解析 JSON 欄位
        test_set_source = {}
        evaluation_metrics = {}
        model_config = {}

        try:
            if row[5]:  # test_set_source
                test_set_source = json.loads(row[5])
        except json.JSONDecodeError:
            pass

        try:
            if row[6]:  # evaluation_metrics
                evaluation_metrics = json.loads(row[6])
        except json.JSONDecodeError:
            pass

        try:
            if row[11]:  # model_config
                model_config = json.loads(row[11])
        except json.JSONDecodeError:
            pass

        # 轉換時間戳記
        from datetime import datetime
        created_at = None
        completed_at = None

        if row[7]:  # created_at
            try:
                if isinstance(row[7], str):
                    created_at = row[7]
                else:
                    created_at = datetime.fromtimestamp(row[7] / 1000).isoformat() + "Z"
            except:
                created_at = str(row[7])

        if row[8]:  # completed_at
            try:
                if isinstance(row[8], str):
                    completed_at = row[8]
                else:
                    completed_at = datetime.fromtimestamp(row[8] / 1000).isoformat() + "Z"
            except:
                completed_at = str(row[8])

        result = {
            "id": row[0],
            "name": row[1],
            "scenario_type": row[2],
            "status": row[3] or "UNKNOWN",
            "trained_model_id": row[4],
            "test_set_source": test_set_source,
            "evaluation_metrics": evaluation_metrics,
            "created_at": created_at,
            "completed_at": completed_at,
            "job_id": row[9],
            "model_info": {
                "name": row[10],
                "config": model_config
            }
        }

        conn.close()

        logger.info(f"Successfully retrieved evaluation run: {run_id}")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching evaluation run {run_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ========== WebSocket Routes ==========

@case_study_v2_router.websocket("/training-jobs/{job_id}/logs")
async def training_logs_websocket(websocket: WebSocket, job_id: str):
    """Streams real-time logs for a specific training job"""
    if not websocket_manager:
        await websocket.close(code=1011, reason="Services not initialized")
        return

    await websocket.accept()

    try:
        await websocket_manager.connect_training_logs(job_id, websocket)

        # Keep connection alive and handle ping/pong
        while True:
            try:
                # Wait for ping from client or disconnect
                message = await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
                if message == "ping":
                    await websocket.send_text("pong")
            except asyncio.TimeoutError:
                # Send ping to check if client is still connected
                await websocket.send_text("ping")
            except WebSocketDisconnect:
                break

    except WebSocketDisconnect:
        pass
    finally:
        await websocket_manager.disconnect_training_logs(job_id, websocket)

@case_study_v2_router.websocket("/evaluation-jobs/{job_id}/logs")
async def evaluation_logs_websocket(websocket: WebSocket, job_id: str):
    """Streams real-time logs for a specific evaluation job"""
    if not websocket_manager:
        await websocket.close(code=1011, reason="Services not initialized")
        return

    await websocket.accept()

    try:
        await websocket_manager.connect_evaluation_logs(job_id, websocket)

        # Keep connection alive and handle ping/pong
        while True:
            try:
                # Wait for ping from client or disconnect
                message = await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
                if message == "ping":
                    await websocket.send_text("pong")
            except asyncio.TimeoutError:
                # Send ping to check if client is still connected
                await websocket.send_text("ping")
            except WebSocketDisconnect:
                break

    except WebSocketDisconnect:
        pass
    finally:
        await websocket_manager.disconnect_evaluation_logs(job_id, websocket)
