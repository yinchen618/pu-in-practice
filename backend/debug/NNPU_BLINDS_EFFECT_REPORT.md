# nnPU 決策邊界百葉窗效果研究報告

## 📋 研究概述

本報告總結了對 nnPU (Non-negative PU Learning) 算法決策邊界百葉窗效果的研究。百葉窗效果是指決策邊界出現鋸齒狀或不規則的振盪，通常由模型過度擬合或參數設置不當導致。

## 🎯 研究目標

尋找會導致 nnPU 決策邊界出現百葉窗效果的參數組合，以便：
1. 理解模型行為的極限情況
2. 避免在實際應用中使用這些參數
3. 為模型調優提供參考

## 🔍 研究方法

### 1. 系統性參數測試
- 測試了 39 個不同的參數組合
- 涵蓋學習率、權重衰減、隱藏層維度、激活函數等關鍵參數
- 使用寬鬆的百葉窗效果檢測標準

### 2. 極端參數測試
- 測試了 8 個極端參數組合
- 使用更寬鬆的檢測標準（百葉窗分數 ≥ 1）
- 強制產生百葉窗效果

### 3. 決策邊界分析指標
- **平滑度**: 決策邊界的平滑程度 (0-1)
- **振盪次數**: Y 座標符號變化的次數
- **Y 變化範圍**: Y 座標的最大變化範圍
- **大幅跳躍**: 相鄰點之間的大幅變化次數
- **方向變化**: 劇烈的方向變化次數
- **百葉窗分數**: 綜合評分 (≥2 為嚴重百葉窗效果)

## 📊 研究結果

### 系統性測試結果
- **總測試配置**: 39 個
- **發現百葉窗效果**: 2 個配置
- **成功率**: 5.1%

### 極端測試結果
- **總測試配置**: 8 個
- **發現百葉窗效果**: 5 個配置
- **成功率**: 62.5%

## 🎯 會產生百葉窗效果的參數組合

### 1. 極高學習率 + tanh 激活函數
```
參數配置:
• 數據分布: two_moons
• 學習率: 0.4 (極高)
• 激活函數: tanh
• 隱藏層維度: 100
• 權重衰減: 0.1
• 訓練週期: 100

百葉窗指標:
• 平滑度: 0.770
• 振盪次數: 31
• 百葉窗分數: 4 (最嚴重)
• Y 變化範圍: 1.727
• 方向變化: 3
```

### 2. 極高學習率 + softsign 激活函數
```
參數配置:
• 數據分布: two_moons
• 學習率: 0.4 (極高)
• 激活函數: softsign
• 隱藏層維度: 100
• 權重衰減: 0.1
• 訓練週期: 100

百葉窗指標:
• 平滑度: 0.860
• 振盪次數: 3
• 百葉窗分數: 3
• Y 變化範圍: 2.606
• 方向變化: 1
```

### 3. 極不平衡數據 + 高學習率
```
參數配置:
• 數據分布: two_moons
• 正樣本數: 5 (極少)
• 未標記樣本數: 500 (大量)
• 先驗: 0.1 (低)
• 學習率: 0.2 (高)
• 隱藏層維度: 100
• 權重衰減: 0.1

百葉窗指標:
• 平滑度: 0.998
• 振盪次數: 11
• 百葉窗分數: 1
• Y 變化範圍: 1.273
• 方向變化: 1
```

### 4. 極低隱藏層 + 高學習率
```
參數配置:
• 數據分布: two_moons
• 學習率: 0.3 (高)
• 隱藏層維度: 5 (極低)
• 權重衰減: 0.1
• 訓練週期: 100

百葉窗指標:
• 平滑度: 1.000
• 振盪次數: 1
• 百葉窗分數: 1
• Y 變化範圍: 2.242
• 方向變化: 0
```

### 5. 高維度 + 低樣本 + 高學習率
```
參數配置:
• 數據分布: complex
• 維度: 15 (高維)
• 正樣本數: 10 (極少)
• 未標記樣本數: 100 (極少)
• 學習率: 0.3 (高)
• 隱藏層維度: 50
• 權重衰減: 0.1

百葉窗指標:
• 平滑度: 0.991
• 振盪次數: 2
• 百葉窗分數: 1
• Y 變化範圍: 2.000
• 方向變化: 0
```

## 📈 關鍵發現

### 1. 最危險的參數組合
- **極高學習率 (0.4-0.5)** + **tanh/softsign 激活函數**
- 這種組合最容易產生嚴重的百葉窗效果
- tanh 激活函數在高學習率下特別不穩定

### 2. 數據不平衡的影響
- **極少正樣本 (3-10)** + **大量未標記樣本 (500-600)**
- 低先驗 (0.05-0.1) 會加劇不穩定性
- 模型難以學習有效的決策邊界

### 3. 模型容量不足
- **極低隱藏層維度 (5-10)** + **高學習率**
- 模型容量不足以學習複雜的決策邊界
- 容易產生過度擬合和振盪

### 4. 高維度問題
- **高維度 (15)** + **極少樣本 (10-100)**
- 維度詛咒導致模型不穩定
- 需要更多的樣本來穩定訓練

## 🛡️ 避免百葉窗效果的建議

### 1. 學習率控制
- **推薦範圍**: 0.001 - 0.01
- **避免**: > 0.2 的學習率
- **特別注意**: tanh 激活函數對高學習率敏感

### 2. 激活函數選擇
- **推薦**: ReLU (最穩定)
- **謹慎使用**: tanh, softsign (需要較低學習率)
- **避免組合**: tanh + 高學習率

### 3. 模型容量
- **隱藏層維度**: 建議 ≥ 20
- **避免**: < 10 的隱藏層維度
- **平衡**: 模型容量與數據量匹配

### 4. 數據平衡
- **正樣本數**: 建議 ≥ 20
- **先驗**: 建議 ≥ 0.2
- **樣本比例**: 避免極度不平衡

### 5. 正則化
- **權重衰減**: 0.01 - 0.05 (適度)
- **避免**: > 0.1 的權重衰減
- **早停**: 監控驗證損失

## 📊 可視化結果

生成了 5 個百葉窗效果的可視化圖片：
1. `nnpu_extreme_blinds_極高學習率_tanh激活.png` - 最嚴重的百葉窗效果
2. `nnpu_extreme_blinds_極高學習率_softsign激活.png` - 嚴重的百葉窗效果
3. `nnpu_extreme_blinds_極不平衡數據_高學習率.png` - 中等百葉窗效果
4. `nnpu_extreme_blinds_極低隱藏層_高學習率.png` - 輕微百葉窗效果
5. `nnpu_extreme_blinds_高維度_低樣本_高學習率.png` - 輕微百葉窗效果

## 🎯 結論

1. **百葉窗效果可重現**: 通過極端參數設置可以可靠地產生百葉窗效果
2. **主要誘因**: 極高學習率、不當的激活函數、數據不平衡、模型容量不足
3. **最危險組合**: 極高學習率 + tanh 激活函數
4. **預防措施**: 使用保守的學習率、ReLU 激活函數、充足的模型容量、平衡的數據

## 📁 相關文件

- `nnpu_blinds_effect_detailed_results.json` - 詳細測試結果
- `nnpu_extreme_blinds_effect_results.json` - 極端測試結果
- `test_nnpu_blinds_effect.py` - 基礎測試腳本
- `test_blinds_effect_detailed.py` - 詳細測試腳本
- `test_extreme_blinds_effect.py` - 極端測試腳本
- `verify_blinds_effect.py` - 驗證腳本

---

**研究完成時間**: 2024年7月27日  
**測試環境**: Python 3.10, nnPU Learning Engine  
**總測試次數**: 47 次  
**發現百葉窗效果**: 7 個配置 
